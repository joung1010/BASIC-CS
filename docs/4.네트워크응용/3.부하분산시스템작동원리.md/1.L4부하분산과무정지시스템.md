# L4 부하분산과 무정지 시스템

## L4 부하분산의 개념

```
                                        15.15.15.15          192.168.0.10 
PC---------- 인터넷------- 인터넷 -------- Web server --------- Web server #1
              │                        www.abc.com         │
PC------------|                                            │ 192.168.0.11
              │                                            -- Web server #2
PC------------|                                            │
              │                                            │ 192.168.0.12
             PC                                            -- Web server #3
                                                           │
                                                           │ 192.168.0.13
                                                           -- Web server #n
```

**공유기가 NAT 기술을 사용하여 사실상 Port까지 변환하고, 이 포트는 TCP, UDP에서의 포트가 되는 것이고 이는 L4 수준의 정보인 것이다. 그래서 이 공유기를 L4 스위치라고 부를 수 있냐? 이게 맞다라고 하기에는 약간 애매한 측면이 존재한다. 스위칭의 개념을 근본적으로 얘기하자면 NAT가 된다고 해서 스위치라고 말하기가 부족한 부분이 있다**.

**L4 스위치는 포트 번호를 기반으로 스위칭이 된다는 말인데, 주로 부하 분산 로드 밸런싱 하는데 사용한다**.

```
NAT vs L4 로드밸런서 비교:

NAT (Network Address Translation):
┌─────────────────────────────────────────────────┐
│ 목적: 주소 변환 및 공유                          │
│ • 내부 → 외부: 사설 IP를 공인 IP로 변환         │
│ • 외부 → 내부: 기존 연결에 대한 응답만 허용     │
│ • 1:N 매핑 (하나의 공인 IP로 여러 내부 IP 지원) │
│                                                 │
│ 특징:                                           │
│ • 클라이언트 보호 중심                          │
│ • 상태 기반 연결 추적                           │
│ • 단방향 연결 시작 (내부 → 외부)               │
└─────────────────────────────────────────────────┘

L4 로드밸런서:
┌─────────────────────────────────────────────────┐
│ 목적: 부하 분산 및 고가용성                     │
│ • 외부 → 내부: 다수의 서버로 트래픽 분산        │
│ • 1:N 매핑 (하나의 VIP로 여러 실제 서버 지원)   │
│ • 서버 상태 모니터링                            │
│                                                 │
│ 특징:                                           │
│ • 서버 보호 및 성능 향상                        │
│ • 헬스체크 기반 트래픽 제어                     │
│ • 양방향 프록시 역할                            │
└─────────────────────────────────────────────────┘

L4 처리 계층:
┌─────────────────────────────────────────────────┐
│ 분석 가능한 정보:                               │
│ • IP 주소 (출발지/목적지)                       │
│ • 포트 번호 (출발지/목적지)                     │
│ • 프로토콜 (TCP/UDP/ICMP)                      │
│ • TCP 플래그 (SYN, ACK, FIN, RST)              │
│                                                 │
│ 분석 불가능한 정보:                             │
│ • HTTP 헤더 (Host, User-Agent)                 │
│ • URL 경로                                     │
│ • 쿠키 정보                                     │
│ • 애플리케이션 데이터                           │
└─────────────────────────────────────────────────┘
```

## 로드밸런서 동작 원리

**위의 PC가 웹서버 abc에 접속한다. 당연히 DNS 서버에서 IP 주소를 받아와서 연결을 한다. 그래서 위의 4대의 PC가 15.15.15.15에 접속을 시도할 것이다. 사실 클라이언트 입장에서 15.15.15.15가 웹서버로 보이지만 사실상 15.15.15.15는 로드밸런서로 작동해서 80번 포트로 들어오면 마치 NAT처럼 포트 포워딩을 하는 것이다. 그래서 이 로드밸런서 뒤에 여러 웹서버가 존재하고 그 서버들의 대리자로서 서버가 응답한 내용을 클라이언트한테 전달하게 된다. 그래서 실제 동작하는 서버는 여러 대일지 모르지만 콘텐츠 내용은 모두 동일하다. 즉 전부 동일한 클론들이고 차이점은 설정된 주소만 다를 뿐이다**.

```
로드밸런서 트래픽 흐름:

클라이언트 관점:
┌─────────────────────────────────────────────────┐
│ DNS 조회: www.abc.com → 15.15.15.15             │
│ 연결 시도: 15.15.15.15:80                       │
│ 인식: 단일 웹서버와 통신 중                      │
│ 실제: 로드밸런서와 통신                         │
└─────────────────────────────────────────────────┘

로드밸런서 처리:
┌─────────────────────────────────────────────────┐
│ 1. 클라이언트 연결 수신:                        │
│   TCP SYN → 15.15.15.15:80                     │
│                                                 │
│ 2. 백엔드 서버 선택:                            │
│   알고리즘에 따라 서버 결정                     │
│   예: 192.168.0.11 선택                        │
│                                                 │
│ 3. 백엔드 연결 설정:                            │
│   TCP SYN → 192.168.0.11:80                    │
│                                                 │
│ 4. 프록시 역할:                                 │
│   클라이언트 ↔ 로드밸런서 ↔ 백엔드 서버        │
└─────────────────────────────────────────────────┘

서버 클러스터 구성:
┌─────────────────────────────────────────────────┐
│ 모든 서버는 동일한 콘텐츠 제공:                  │
│ • 동일한 웹 애플리케이션                        │
│ • 동일한 데이터베이스 접근                      │
│ • 동일한 정적 파일                              │
│ • 동기화된 설정                                 │
│                                                 │
│ 차이점:                                         │
│ • IP 주소: 192.168.0.10, .11, .12, .13        │
│ • 호스트명: web01, web02, web03, web04         │
│ • 로그 파일 위치                                │
└─────────────────────────────────────────────────┘
```

## 부하분산 알고리즘

**로드밸런싱을 한다는 것은 부하가 오면 접속해서 처리되어야 할 연산이 있다면 여러 대의 서버 중에 하나와 매핑해서 처리한다. 그래서 클라이언트가 접속을 요청할 때 순서대로 서버를 매핑해주는 단순한 방식을 라운드로빈 방식이라고 한다**.

```
주요 로드밸런싱 알고리즘:

Round Robin (라운드로빈):
┌─────────────────────────────────────────────────┐
│ 순차적 분배:                                    │
│ 요청1 → 서버1, 요청2 → 서버2, 요청3 → 서버3     │
│ 요청4 → 서버1, 요청5 → 서버2, ...              │
│                                                 │
│ 장점: 구현 간단, 균등 분배                      │
│ 단점: 서버 성능 차이 무시                       │
└─────────────────────────────────────────────────┘

Weighted Round Robin (가중 라운드로빈):
┌─────────────────────────────────────────────────┐
│ 서버별 가중치 적용:                             │
│ 서버1 (가중치 3): 3개 요청                      │
│ 서버2 (가중치 2): 2개 요청                      │
│ 서버3 (가중치 1): 1개 요청                      │
│                                                 │
│ 분배 패턴: 1→1→2→1→2→3→1→1→2→...              │
└─────────────────────────────────────────────────┘

Least Connections (최소 연결):
┌─────────────────────────────────────────────────┐
│ 현재 연결 수가 가장 적은 서버 선택:              │
│ 서버1: 10개 연결                                │
│ 서버2: 5개 연결 ← 선택                         │
│ 서버3: 8개 연결                                │
│                                                 │
│ 장점: 실시간 부하 반영                          │
│ 단점: 연결 추적 오버헤드                        │
└─────────────────────────────────────────────────┘

IP Hash (IP 해시):
┌─────────────────────────────────────────────────┐
│ 클라이언트 IP 기반 일관된 라우팅:                │
│ hash(client_ip) % server_count                  │
│                                                 │
│ 1.2.3.4 → 항상 서버1                          │
│ 5.6.7.8 → 항상 서버2                          │
│                                                 │
│ 장점: 세션 지속성                               │
│ 단점: 불균등 분배 가능성                        │
└─────────────────────────────────────────────────┘

Resource Based (리소스 기반):
┌─────────────────────────────────────────────────┐
│ 실시간 서버 상태 기반 선택:                     │
│ • CPU 사용률                                   │
│ • 메모리 사용률                                 │
│ • 응답 시간                                     │
│ • 처리량                                        │
│                                                 │
│ 가장 여유로운 서버로 라우팅                     │
└─────────────────────────────────────────────────┘
```

## 헬스체크와 장애 탐지

근데 **보통 부하분산이라고 하면 이러한 단순한 것보다는 반드시 Health Check라는 기능이 추가되어야 한다**.

```
                            15.15.15.15          192.168.0.10 
     인터넷-------    LB   --------- Web server #1
                            www.abc.com         │
                                                │ 192.168.0.11
                                                -- Web server #2
                                                │
                                                │ 192.168.0.12
                                                -- Web server #3
                                                │
                                                │ 192.168.0.13
                                                -- Web server #n
                                                
Manager 서버: LB 포함 모든 n대의 서버와 연결된 서버
```

**평소에 헬스체크를 한다는 말은 소위 부하를 일으키는 것 중에 가장 대표적인 것은 CPU 사용량 그다음에 이제 메모리 사용량, HDD와 같은 공간의 여유공간 등 시스템이 정상적인 운영상태를 점검할 수 있게 측정할 수 있는 이러한 것들을 이 매니저라는 서버가 항상 이 부하율을 계산하고 있다. 그래서 이러한 수치들이 일정 수준 이상으로 올라가는 것을 감지해서 해당 서버가 현재 부하분산 대상서버로 적정하지 않다고 보는 것이다**.

```
헬스체크 메커니즘:

Active Health Check (능동적 헬스체크):
┌─────────────────────────────────────────────────┐
│ 로드밸런서가 주기적으로 서버 상태 확인:          │
│                                                 │
│ HTTP Health Check:                              │
│ GET /health HTTP/1.1                           │
│ Host: 192.168.0.10                             │
│ → 응답: 200 OK (정상) / 5xx (비정상)           │
│                                                 │
│ TCP Health Check:                              │
│ TCP 연결 시도 → 192.168.0.10:80               │
│ → 연결 성공 (정상) / 연결 실패 (비정상)        │
│                                                 │
│ ICMP Ping:                                     │
│ ping 192.168.0.10                             │
│ → 응답 있음 (정상) / 응답 없음 (비정상)        │
└─────────────────────────────────────────────────┘

Passive Health Check (수동적 헬스체크):
┌─────────────────────────────────────────────────┐
│ 실제 트래픽 응답을 통한 상태 판단:               │
│                                                 │
│ 응답 시간 모니터링:                             │
│ • 정상: 100ms 이하                             │
│ • 경고: 100-500ms                              │
│ • 비정상: 500ms 이상                           │
│                                                 │
│ 에러율 모니터링:                                │
│ • 정상: 에러율 1% 이하                         │
│ • 경고: 에러율 1-5%                            │
│ • 비정상: 에러율 5% 이상                       │
└─────────────────────────────────────────────────┘

시스템 리소스 모니터링:
┌─────────────────────────────────────────────────┐
│ CPU 사용률:                                     │
│ • 정상: 70% 이하                               │
│ • 경고: 70-85%                                 │
│ • 위험: 85% 이상                               │
│                                                 │
│ 메모리 사용률:                                  │
│ • 정상: 80% 이하                               │
│ • 경고: 80-90%                                 │
│ • 위험: 90% 이상                               │
│                                                 │
│ 디스크 사용률:                                  │
│ • 정상: 85% 이하                               │
│ • 경고: 85-95%                                 │
│ • 위험: 95% 이상                               │
│                                                 │
│ 네트워크 I/O:                                  │
│ • 대역폭 사용률                                 │
│ • 패킷 손실률                                   │
│ • 지연 시간                                     │
└─────────────────────────────────────────────────┘
```

## 장애 격리와 무정지 시스템

그래서 **PC에서 접속 요청이 오면 LB에서 바로 다른 서버에게 트래픽을 보내는 것이 아니라 먼저 매니저한테 현재 가장 여유로운 서버가 3번이야 3번으로 보내와 같이 트래픽을 보낼 서버를 지정받는다**.

근데 **이때 1번 서버가 지속적으로 CPU 사용률이 80퍼센트를 넘게 유지하고 있다면 매니저 입장에서는 이 서버 1번을 격리시켜 버리면 된다. 이 1번을 스케줄링에서 제외해주면 된다. 그래서 이 1번을 AS를 보내고 새로운 서버를 할당해서 처리한다**.

**여기서 중요한 점은 이 1번 서버가 어떠한 장애로 사라져도 실제 15.15.15.15로 접속하는 클라이언트들은 이 웹서버 1번이 멈췄다고 생각하지 않는다. 이러한 시스템을 무정지 시스템, 폴트톨러런트 시스템이라고 한다**.

```
장애 대응 시나리오:

서버 장애 탐지:
┌─────────────────────────────────────────────────┐
│ 1. 헬스체크 실패 감지:                          │
│   연속 3회 응답 실패 → 서버1 비정상 판정        │
│                                                 │
│ 2. 즉시 트래픽 차단:                            │
│   서버1을 로드밸런싱 풀에서 제외                │
│   신규 연결을 서버2, 3, 4로만 분배             │
│                                                 │
│ 3. 기존 연결 처리:                              │
│   Graceful: 기존 세션 완료 후 종료              │
│   Immediate: 즉시 연결 종료 후 재연결           │
└─────────────────────────────────────────────────┘

자동 복구:
┌─────────────────────────────────────────────────┐
│ 1. 자동 재시작 시도:                            │
│   서비스 프로세스 재시작                        │
│   시스템 재부팅 (필요시)                        │
│                                                 │
│ 2. 헬스체크 재개:                               │
│   서버1 상태 지속 모니터링                      │
│   연속 3회 성공 → 정상 복구 판정                │
│                                                 │
│ 3. 트래픽 복구:                                 │
│   서버1을 다시 로드밸런싱 풀에 추가             │
│   점진적 트래픽 증가 (Warm-up)                  │
└─────────────────────────────────────────────────┘

클라이언트 관점:
┌─────────────────────────────────────────────────┐
│ 서비스 연속성:                                  │
│ • 서버1 장애 → 서버2로 자동 재연결              │
│ • 사용자는 장애 인지 못함                       │
│ • 서비스 중단 시간 최소화                       │
│                                                 │
│ 성능 영향:                                      │
│ • 일시적 응답 지연 가능                         │
│ • 전체 처리 용량 일시 감소                      │
│ • 복구 후 정상 성능 회복                        │
└─────────────────────────────────────────────────┘
```

## 고가용성과 이중화

**만약 로드밸런서만 죽고 나머지 서버들이 살아있다면 이 또한 큰일이다. 그래서 보통은 부하분산 장치가 하나 더 있어서 외부에서는 마치 하나처럼 동작한다. 즉 이중화 처리가 되어 있다. 그래서 로드밸런서끼리 연결되어 있어서 한쪽의 상태가 다운되어도 다른 한쪽에서 이를 처리한다**.

```
로드밸런서 이중화 구성:

Active-Passive 구성:
┌─────────────────────────────────────────────────┐
│          VIP: 15.15.15.15                       │
│                    │                            │
│         ┌──────────┼──────────┐                 │
│         │                     │                 │
│    LB1 (Active)         LB2 (Passive)           │
│   192.168.1.10        192.168.1.11             │
│         │                     │                 │
│         └──── Heartbeat ──────┘                 │
│                                                 │
│ 정상시: LB1이 모든 트래픽 처리                   │
│ 장애시: LB2가 VIP 인계받아 서비스 계속          │
└─────────────────────────────────────────────────┘

Active-Active 구성:
┌─────────────────────────────────────────────────┐
│     VIP1: 15.15.15.15    VIP2: 15.15.15.16     │
│           │                      │              │
│    LB1 (Active)           LB2 (Active)          │
│   192.168.1.10          192.168.1.11           │
│           │                      │              │
│           └──── Heartbeat ──────┘              │
│                                                 │
│ 정상시: 두 LB가 트래픽 분담                     │
│ 장애시: 생존 LB가 모든 트래픽 처리              │
└─────────────────────────────────────────────────┘

VRRP (Virtual Router Redundancy Protocol):
┌─────────────────────────────────────────────────┐
│ 1. 가상 IP 관리:                               │
│   두 LB가 하나의 가상 IP 공유                   │
│   Master/Backup 역할 자동 할당                 │
│                                                 │
│ 2. 헬스체크:                                    │
│   주기적 Heartbeat 메시지 교환                  │
│   Master 장애 시 Backup이 즉시 인계             │
│                                                 │
│ 3. 페일오버:                                    │
│   • 감지 시간: 1-3초                           │
│   • 전환 시간: 1-2초                           │
│   • 총 중단 시간: 3-5초                        │
└─────────────────────────────────────────────────┘
```

## 현대적 구현: 컨테이너와 오케스트레이션

그래서 **최근에는 이 서버들이 전부 VM이라면 장애 시 그냥 그 VM만 재시작이나 가상 이미지를 되돌리면 되는 것이고 그리고 이 서버들의 원본 VM이 있어서 보안패치를 원본 VM에 적용하고 이를 다른 VM들에게 내리면 되는 것이다. 그래서 이러한 부분들에 대한 자동화 고민 때문에 급부상하게 되는 기술이 Docker이다. 그리고 이러한 것들을 관리적인 측면에서 자동화해야 되기 때문에 Kubernetes 같은 것이 성장하는 것이다**.

```
전통적 vs 현대적 아키텍처:

전통적 물리 서버:
┌─────────────────────────────────────────────────┐
│ 문제점:                                         │
│ • 서버 프로비저닝 시간: 수 시간~수 일           │
│ • 리소스 낭비: 평균 활용률 10-20%               │
│ • 확장성 제한: 물리적 한계                      │
│ • 관리 복잡성: 개별 서버 관리                   │
│                                                 │
│ 장애 복구:                                      │
│ • 하드웨어 교체 시간                            │
│ • 수동 설정 복구                                │
│ • 서비스 중단 시간 길어짐                       │
└─────────────────────────────────────────────────┘

가상화 (VM):
┌─────────────────────────────────────────────────┐
│ 개선점:                                         │
│ • 프로비저닝 시간: 수 분                        │
│ • 리소스 효율성: 60-80% 활용률                  │
│ • 스냅샷/복원: 빠른 롤백                        │
│ • 템플릿 활용: 표준화된 배포                    │
│                                                 │
│ 장애 복구:                                      │
│ • VM 재시작/이미지 복원                         │
│ • 자동화된 배포 스크립트                        │
│ • 상대적으로 빠른 복구                          │
└─────────────────────────────────────────────────┘

컨테이너 (Docker):
┌─────────────────────────────────────────────────┐
│ 혁신적 개선:                                    │
│ • 시작 시간: 수 초                             │
│ • 리소스 효율성: 80-95% 활용률                  │
│ • 이미지 기반: 일관된 환경                      │
│ • 마이크로서비스: 세밀한 확장                   │
│                                                 │
│ 장애 복구:                                      │
│ • 즉시 컨테이너 재시작                          │
│ • 불변 인프라: 항상 동일한 상태                 │
│ • 자동 스케일링                                 │
└─────────────────────────────────────────────────┘

Kubernetes 오케스트레이션:
┌─────────────────────────────────────────────────┐
│ 자동화된 관리:                                  │
│ • 자동 배포 (Rolling Update)                   │
│ • 자동 스케일링 (HPA, VPA)                     │
│ • 자동 복구 (Self-healing)                     │
│ • 로드밸런싱 (Service, Ingress)                │
│                                                 │
│ 선언적 구성:                                    │
│ • YAML 매니페스트로 원하는 상태 정의            │
│ • 컨트롤러가 지속적으로 상태 유지               │
│ • GitOps: 코드로 인프라 관리                   │
└─────────────────────────────────────────────────┘
```

## Docker의 네트워크 격리 메커니즘

Docker는 Linux Kernel의 Network Namespace 기능을 사용해서 완전한 네트워크 격리를 구현합니다. 각 컨테이너는 독립적인 네트워크 스택을 가지는데, 이는 별도의 컴퓨터가 가지는 네트워크 환경과 동일합니다.

**컨테이너가 생성될 때 실제로 일어나는 일**은 다음과 같습니다. Linux 커널이 새로운 Network Namespace를 생성하고, 이 네임스페이스 안에는 독립적인 네트워크 인터페이스 목록, 라우팅 테이블, ARP 테이블, iptables 규칙, 소켓 테이블이 생성됩니다.

```
컨테이너별 독립적 네트워크 스택:

Host OS
├── Container 1 (Namespace 1)
│   ├── eth0: 172.17.0.2/16
│   ├── 독립적인 라우팅 테이블
│   ├── 독립적인 ARP 테이블
│   └── 독립적인 iptables 규칙
├── Container 2 (Namespace 2)
│   ├── eth0: 172.17.0.3/16
│   ├── 독립적인 라우팅 테이블
│   └── 독립적인 ARP 테이블
└── Host Network (기본 네임스페이스)
    ├── docker0: 172.17.0.1/16
    └── eth0: 192.168.1.100
```

**이는 앞서 학습한 NAT 환경의 내부 PC들과 근본적으로 다릅니다**. NAT 뒤의 PC들은 물리적으로 분리된 컴퓨터이지만, Docker 컨테이너는 같은 호스트 OS 커널 위에서 논리적으로만 분리된 환경입니다.

## veth pair의 동작 원리

Docker가 컨테이너를 외부 네트워크에 연결하기 위해 사용하는 것이 veth pair입니다. **이는 가상의 이더넷 케이블 한 쌍**으로 생각할 수 있습니다.

veth pair는 두 개의 가상 네트워크 인터페이스로 구성되는데, 한쪽 끝에서 패킷을 넣으면 다른 쪽 끝에서 패킷이 나옵니다. Docker는 veth pair의 한쪽을 컨테이너 네임스페이스 안에 넣고 이름을 eth0으로 바꿉니다. 다른 쪽은 호스트 네임스페이스에 남겨두고 docker0 브리지에 연결합니다.

```
veth pair 연결 구조:

Container Namespace          Host Namespace
┌─────────────────┐         ┌──────────────────┐
│      eth0       │◄────────┤   veth12345678   │
│  172.17.0.2     │  가상   │    (no IP)       │
└─────────────────┘  케이블  └─────────┬────────┘
                                      │
                             ┌────────▼────────┐
                             │    docker0      │
                             │   172.17.0.1    │
                             └─────────────────┘
```

**이 구조가 중요한 이유**는 컨테이너에서 발생한 모든 네트워크 트래픽이 반드시 호스트를 거쳐야 한다는 점입니다. 컨테이너는 직접 물리 네트워크에 접근할 수 없고, 모든 통신이 호스트의 네트워크 스택을 통해 이루어집니다.

## docker0 브리지의 다중 역할

docker0 브리지는 단순한 L2 스위치가 아닙니다. **세 가지 핵심 역할을 동시에 수행**합니다.

**첫 번째는 L2 스위치 역할**입니다. 같은 docker0 브리지에 연결된 컨테이너들 간의 통신에서 MAC 주소 학습과 프레임 포워딩을 담당합니다. 컨테이너1이 컨테이너2와 통신할 때, docker0가 ARP 브로드캐스트를 전달하고 MAC 주소를 학습해서 이후 직접 프레임을 포워딩합니다.

**두 번째는 기본 게이트웨이 역할**입니다. 모든 컨테이너의 기본 라우팅 규칙은 "default via 172.17.0.1"인데, 이 172.17.0.1이 바로 docker0 브리지의 IP입니다. 컨테이너에서 외부로 나가는 모든 패킷은 docker0를 거쳐 호스트의 네트워크 스택으로 전달됩니다.

**세 번째는 DHCP 서버 역할**입니다. Docker daemon이 IPAM(IP Address Management)을 통해 172.17.0.2부터 시작해서 새로운 컨테이너마다 순차적으로 IP를 할당합니다. 컨테이너가 삭제되면 해당 IP는 다시 풀로 반환됩니다.

## NAT 변환의 정확한 과정

Docker에서 컨테이너가 외부와 통신할 때 일어나는 NAT 변환 과정을 살펴보겠습니다. 이는 앞서 학습한 공유기의 NAT와 동일한 메커니즘입니다.

**컨테이너에서 google.com에 접속하는 상황**을 가정해보겠습니다. 먼저 컨테이너 내부에서 애플리케이션이 socket() 시스템 콜을 호출하고 connect(8.8.8.8, 80)를 실행합니다. 이때 컨테이너의 TCP/IP 스택이 출발지 IP를 172.17.0.2, 목적지 IP를 8.8.8.8로 하는 패킷을 생성합니다.

컨테이너의 라우팅 테이블에는 "default via 172.17.0.1"이 설정되어 있으므로, 패킷은 docker0 브리지로 전송됩니다. 이때 ARP 프로토콜을 통해 172.17.0.1의 MAC 주소를 얻어야 하는데, docker0 브리지가 ARP 응답을 보냅니다.

패킷이 veth pair를 통해 호스트 네트워크 스택에 도달하면, 호스트의 라우팅 테이블을 조회합니다. 8.8.8.8는 외부 주소이므로 기본 게이트웨이로 전송되어야 합니다.

```
NAT 변환 과정:

1. 컨테이너 → docker0:
   Src: 172.17.0.2:45678, Dst: 8.8.8.8:80

2. iptables POSTROUTING 처리:
   규칙: -A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE

3. NAT 변환 후:
   Src: 192.168.1.100:54321, Dst: 8.8.8.8:80

4. NAT 테이블 기록:
   172.17.0.2:45678 ↔ 192.168.1.100:54321
```

**이때 중요한 점은 iptables의 POSTROUTING 체인에서 MASQUERADE 규칙이 적용된다는 것**입니다. 이 규칙은 172.17.0.0/16 대역에서 나오는 패킷의 출발지 IP를 호스트의 IP로 변경합니다. 동시에 출발지 포트도 호스트의 사용 가능한 포트로 변경하고, 이 매핑 정보를 NAT 테이블에 저장합니다.

응답 패킷이 돌아올 때는 역과정이 일어납니다. Google 서버에서 192.168.1.100:54321로 응답을 보내면, iptables의 connection tracking 기능이 NAT 테이블을 조회해서 이 패킷이 172.17.0.2:45678로 가야 한다는 것을 알아냅니다. 그리고 DNAT를 적용해서 목적지 주소를 변경한 후 docker0 브리지를 통해 컨테이너로 전달합니다.

## Docker 포트 매핑의 구현

`docker run -p 8080:80 nginx` 명령어가 실행되면 Docker daemon이 자동으로 iptables 규칙을 추가합니다. 이는 우리가 앞서 학습한 포트 포워딩과 정확히 같은 메커니즘입니다.

**Docker가 생성하는 iptables 규칙**은 세 가지입니다. 첫 번째는 PREROUTING 체인의 DNAT 규칙입니다. 이 규칙은 호스트의 8080 포트로 들어오는 모든 TCP 패킷을 172.17.0.2:80으로 변환합니다.

두 번째는 FORWARD 체인의 ACCEPT 규칙입니다. 이는 172.17.0.2:80으로 가는 패킷의 통과를 허용합니다. Docker는 기본적으로 컨테이너로의 직접 접근을 차단하므로, 포트 매핑된 포트에 대해서만 예외적으로 허용하는 것입니다.

세 번째는 POSTROUTING 체인의 MASQUERADE 규칙입니다. 이는 같은 호스트 내에서 컨테이너에 접근할 때 필요한 규칙입니다.

```
포트 매핑 패킷 흐름:

외부 클라이언트 → 192.168.1.100:8080
        ↓
   PREROUTING DNAT 적용
        ↓
   목적지 변경: 172.17.0.2:80
        ↓
   docker0 브리지 → veth pair
        ↓
   컨테이너 nginx 서버
```

**실제 패킷 흐름을 보면**, 외부 클라이언트가 192.168.1.100:8080으로 HTTP 요청을 보냅니다. 이 패킷이 호스트에 도착하면 iptables의 PREROUTING 체인에서 DNAT 규칙이 적용되어 목적지가 172.17.0.2:80으로 변경됩니다. 그 후 호스트의 라우팅 테이블을 조회해서 172.17.0.0/16 대역은 docker0로 가야 한다는 것을 확인하고, docker0 브리지가 ARP 테이블을 조회해서 해당 veth 인터페이스로 패킷을 전달합니다.

## 컨테이너 간 통신

같은 호스트의 컨테이너들 간 통신은 docker0 브리지를 통해 L2 레벨에서 직접 이루어집니다. 이는 물리적인 스위치에 연결된 PC들 간의 통신과 동일한 방식입니다.

**컨테이너1에서 컨테이너2로 패킷을 보내는 과정**을 살펴보겠습니다. 먼저 컨테이너1이 목적지 IP 172.17.0.3에 대한 ARP 요청을 브로드캐스트합니다. 이 ARP 요청은 veth pair를 통해 docker0 브리지에 도달하고, 브리지는 연결된 모든 포트로 이 ARP 요청을 전달합니다.

컨테이너2가 ARP 응답을 보내면, docker0 브리지는 컨테이너2의 MAC 주소와 포트 정보를 학습합니다. 이후 컨테이너1에서 컨테이너2로 보내는 모든 프레임은 docker0 브리지가 직접 해당 포트로 전달합니다.

**중요한 점은 이 통신에서 NAT 변환이 일어나지 않는다는 것**입니다. 같은 서브넷 내의 통신이므로 L3 라우팅이 필요 없고, L2 스위칭만으로 통신이 가능합니다. 따라서 지연시간이 최소화되고 처리량도 높습니다.

## Docker 네트워크 드라이버

Docker는 다양한 네트워크 드라이버를 제공해서 각기 다른 네트워킹 요구사항을 만족시킵니다.

**기본 Bridge 네트워크**는 지금까지 설명한 docker0 브리지를 사용하는 방식입니다. 이는 단일 호스트 내의 컨테이너들을 연결하고 NAT를 통해 외부와 통신합니다.

**Host 네트워크**는 컨테이너가 호스트의 네트워크 네임스페이스를 직접 사용하는 방식입니다. 이 경우 네트워크 격리가 없어지지만 성능이 최대가 됩니다. 컨테이너가 호스트의 모든 네트워크 인터페이스에 직접 접근할 수 있고, 포트 충돌에 주의해야 합니다.

**None 네트워크**는 컨테이너에 네트워크 인터페이스를 전혀 제공하지 않는 방식입니다. 루프백 인터페이스만 존재하고 외부 통신이 불가능합니다.

**Overlay 네트워크**는 Docker Swarm 모드에서 사용되는 다중 호스트 네트워킹입니다. VXLAN 터널링을 사용해서 서로 다른 호스트의 컨테이너들이 마치 같은 네트워크에 있는 것처럼 통신할 수 있게 합니다.

```
네트워크 드라이버 비교:

Bridge: 컨테이너 ← NAT → 외부
Host:   컨테이너 = 호스트 네트워크
None:   컨테이너 (격리, 외부 통신 불가)
Overlay: 호스트A 컨테이너 ← VXLAN → 호스트B 컨테이너
```

이러한 Docker의 네트워킹 메커니즘은 결국 지금까지 학습한 모든 네트워크 개념들의 조합입니다. Network Namespace를 통한 격리, veth pair를 통한 가상 연결, Linux Bridge를 통한 L2 스위칭, iptables를 통한 NAT 변환과 포트 포워딩이 모두 유기적으로 결합되어 컨테이너 네트워킹을 구현합니다.